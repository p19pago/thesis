{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36cef4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thesis Topic\n",
    "# Create a Decision Support System (DSS) which will support datasets regarding breast cancer.\n",
    "# All required to be done, is import all necessary modules, required for the system designed.\n",
    "# To begin with, the built-in CSV module will be imported.\n",
    "\n",
    "import csv\n",
    "\n",
    "# Then the \"rest\" of the modules required, are imported (and explained) one-by-one.\n",
    "# The numpy module may be useful for experiments with the scikit-learn module,\n",
    "# which will be imported later on.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Another \"useful\" package (in case of emergency) ought to be the math package\n",
    "# just as the CSV module, it is built-in in Python.\n",
    "\n",
    "import math\n",
    "\n",
    "# And then, the pandas module will be imported.\n",
    "# The pandas module will be of need for the Decision Support System designed and implemented,\n",
    "# as well as the CSV datasets the program will fetch to be read.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Most experiments, will base on machine learning and data science.\n",
    "# The scikit-learn module will be called and imported using the \"sklearn\" abbreviation\n",
    "# and by importing it, all features will be granted access.\n",
    "\n",
    "import sklearn\n",
    "\n",
    "# The \"random\" package is imported.\n",
    "# Occasionally, some sequences may feature random numbers and/or time.\n",
    "# Therefore, it is considered useful.\n",
    "\n",
    "import random\n",
    "\n",
    "# For more complicated algorithms, classifiers and distributions, the \"scipy\" package might be of need;\n",
    "# therefore, it will be imported, and there will be high chance of use in the model.\n",
    "\n",
    "# the scipy package will be imported with all its features.\n",
    "\n",
    "import scipy\n",
    "\n",
    "# Lastly, for the plot part of the model,\n",
    "# the \"pyplot\" and \"seaborn\" packages are imported.\n",
    "# Both may consider useful for the model, because later they may be used to generate and display figures.\n",
    "\n",
    "# First, importing the \"pyplot\" package for basic plots and histograms.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For advanced plotting and heatmaps, it is useful to import the \"seaborn\" package.\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d6fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the scikit-learn package has been imported (see above, on the first cell)\n",
    "# some extra features, coming from this package, will optionally be imported, \n",
    "# in order for them to be used, and will be used in the current model.\n",
    "\n",
    "# For instance, the train and test split, will be used, \n",
    "# as long as each dataset will be trained and tested.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Same thing with cross-validation models and metrics.\n",
    "# Such as the K-Fold Cross Validation process, which will be used later on the model,\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# and one for K-Fold process\n",
    "# in this case, a Stratified K-Fold method will be used.\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# feature selection techniques,\n",
    "# as well as classification techniques, \n",
    "# used for later contributions to the model\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "\n",
    "# additional module for the decision tree classifier\n",
    "# (in case it is done separately)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# import the Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# will be added\n",
    "\n",
    "# as well as extras such as matrices\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "# the command above, will import necessary metrics on the model\n",
    "# optionally, the confusion matrix metric will be imported (to avoid errors)\n",
    "# and so the same with classification report metric\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# will be added\n",
    "\n",
    "# Lastly, the XGBoost classifier will be imported, due to data classification\n",
    "# in later contributions to the model.\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee402c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an attempt for reading the second dataset will each be made.\n",
    "# In the below line of code, the dataR2.csv file will be stored into a dataframe, using a method from the pandas package.\n",
    "# The dataframe for the second dataset, will be named into df_d2.\n",
    "# It will be used for the second dataset given.\n",
    "\n",
    "df_d2 = pd.read_csv('C:\\\\Users\\\\user\\\\Documents\\\\thesis\\\\files\\\\dataR2.csv',\n",
    "                       sep=\",\",\n",
    "                        decimal=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142a6eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>HOMA</th>\n",
       "      <th>Leptin</th>\n",
       "      <th>Adiponectin</th>\n",
       "      <th>Resistin</th>\n",
       "      <th>MCP.1</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>70</td>\n",
       "      <td>2.707</td>\n",
       "      <td>0.467409</td>\n",
       "      <td>8.8071</td>\n",
       "      <td>9.702400</td>\n",
       "      <td>7.99585</td>\n",
       "      <td>417.114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>20.690495</td>\n",
       "      <td>92</td>\n",
       "      <td>3.115</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>5.429285</td>\n",
       "      <td>4.06405</td>\n",
       "      <td>468.786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>23.124670</td>\n",
       "      <td>91</td>\n",
       "      <td>4.498</td>\n",
       "      <td>1.009651</td>\n",
       "      <td>17.9393</td>\n",
       "      <td>22.432040</td>\n",
       "      <td>9.27715</td>\n",
       "      <td>554.697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>21.367521</td>\n",
       "      <td>77</td>\n",
       "      <td>3.226</td>\n",
       "      <td>0.612725</td>\n",
       "      <td>9.8827</td>\n",
       "      <td>7.169560</td>\n",
       "      <td>12.76600</td>\n",
       "      <td>928.220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>92</td>\n",
       "      <td>3.549</td>\n",
       "      <td>0.805386</td>\n",
       "      <td>6.6994</td>\n",
       "      <td>4.819240</td>\n",
       "      <td>10.57635</td>\n",
       "      <td>773.920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age        BMI  Glucose  Insulin      HOMA   Leptin  Adiponectin  Resistin  \\\n",
       "0   48  23.500000       70    2.707  0.467409   8.8071     9.702400   7.99585   \n",
       "1   83  20.690495       92    3.115  0.706897   8.8438     5.429285   4.06405   \n",
       "2   82  23.124670       91    4.498  1.009651  17.9393    22.432040   9.27715   \n",
       "3   68  21.367521       77    3.226  0.612725   9.8827     7.169560  12.76600   \n",
       "4   86  21.111111       92    3.549  0.805386   6.6994     4.819240  10.57635   \n",
       "\n",
       "     MCP.1  Classification  \n",
       "0  417.114               1  \n",
       "1  468.786               1  \n",
       "2  554.697               1  \n",
       "3  928.220               1  \n",
       "4  773.920               1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before getting info for the second dataset, the first five rows of the dataset will be read.\n",
    "# It is an optional part; however, it will be useful to get necessary information for the dataset\n",
    "# and its later procedures.\n",
    "\n",
    "df_d2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056c9ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116 entries, 0 to 115\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             116 non-null    int64  \n",
      " 1   BMI             116 non-null    float64\n",
      " 2   Glucose         116 non-null    int64  \n",
      " 3   Insulin         116 non-null    float64\n",
      " 4   HOMA            116 non-null    float64\n",
      " 5   Leptin          116 non-null    float64\n",
      " 6   Adiponectin     116 non-null    float64\n",
      " 7   Resistin        116 non-null    float64\n",
      " 8   MCP.1           116 non-null    float64\n",
      " 9   Classification  116 non-null    int64  \n",
      "dtypes: float64(7), int64(3)\n",
      "memory usage: 9.2 KB\n"
     ]
    }
   ],
   "source": [
    "# Before printing the second dataframe, as long as the first one has been shown and printed\n",
    "# it is optional to show info of the second dataframe.\n",
    "\n",
    "# The second dataframe, follows as it is.\n",
    "# The procedure, will remain as the first.\n",
    "\n",
    "# It will display necessary information and content\n",
    "# related to all of the second dataframe, \n",
    "# first five rows, having been imported just above.\n",
    "\n",
    "df_d2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e915409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age        BMI  Glucose  Insulin      HOMA   Leptin  Adiponectin  \\\n",
      "0     48  23.500000       70    2.707  0.467409   8.8071     9.702400   \n",
      "1     83  20.690495       92    3.115  0.706897   8.8438     5.429285   \n",
      "2     82  23.124670       91    4.498  1.009651  17.9393    22.432040   \n",
      "3     68  21.367521       77    3.226  0.612725   9.8827     7.169560   \n",
      "4     86  21.111111       92    3.549  0.805386   6.6994     4.819240   \n",
      "..   ...        ...      ...      ...       ...      ...          ...   \n",
      "111   45  26.850000       92    3.330  0.755688  54.6800    12.100000   \n",
      "112   62  26.840000      100    4.530  1.117400  12.4500    21.420000   \n",
      "113   65  32.050000       97    5.730  1.370998  61.4800    22.540000   \n",
      "114   72  25.590000       82    2.820  0.570392  24.9600    33.750000   \n",
      "115   86  27.180000      138   19.910  6.777364  90.2800    14.110000   \n",
      "\n",
      "     Resistin    MCP.1  Classification  \n",
      "0     7.99585  417.114               1  \n",
      "1     4.06405  468.786               1  \n",
      "2     9.27715  554.697               1  \n",
      "3    12.76600  928.220               1  \n",
      "4    10.57635  773.920               1  \n",
      "..        ...      ...             ...  \n",
      "111  10.96000  268.230               2  \n",
      "112   7.32000  330.160               2  \n",
      "113  10.33000  314.050               2  \n",
      "114   3.27000  392.460               2  \n",
      "115   4.35000   90.090               2  \n",
      "\n",
      "[116 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Later, the dataset will be printed on the screen\n",
    "# regarding the content received above.\n",
    "\n",
    "print (df_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefe69a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>HOMA</th>\n",
       "      <th>Leptin</th>\n",
       "      <th>Adiponectin</th>\n",
       "      <th>Resistin</th>\n",
       "      <th>MCP.1</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>116.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>116.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>57.301724</td>\n",
       "      <td>27.582111</td>\n",
       "      <td>97.793103</td>\n",
       "      <td>10.012086</td>\n",
       "      <td>2.694988</td>\n",
       "      <td>26.615080</td>\n",
       "      <td>10.180874</td>\n",
       "      <td>14.725966</td>\n",
       "      <td>534.647000</td>\n",
       "      <td>1.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.112766</td>\n",
       "      <td>5.020136</td>\n",
       "      <td>22.525162</td>\n",
       "      <td>10.067768</td>\n",
       "      <td>3.642043</td>\n",
       "      <td>19.183294</td>\n",
       "      <td>6.843341</td>\n",
       "      <td>12.390646</td>\n",
       "      <td>345.912663</td>\n",
       "      <td>0.499475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>18.370000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>2.432000</td>\n",
       "      <td>0.467409</td>\n",
       "      <td>4.311000</td>\n",
       "      <td>1.656020</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>45.843000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>22.973205</td>\n",
       "      <td>85.750000</td>\n",
       "      <td>4.359250</td>\n",
       "      <td>0.917966</td>\n",
       "      <td>12.313675</td>\n",
       "      <td>5.474283</td>\n",
       "      <td>6.881763</td>\n",
       "      <td>269.978250</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>27.662416</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.924500</td>\n",
       "      <td>1.380939</td>\n",
       "      <td>20.271000</td>\n",
       "      <td>8.352692</td>\n",
       "      <td>10.827740</td>\n",
       "      <td>471.322500</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>71.000000</td>\n",
       "      <td>31.241442</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>11.189250</td>\n",
       "      <td>2.857787</td>\n",
       "      <td>37.378300</td>\n",
       "      <td>11.815970</td>\n",
       "      <td>17.755207</td>\n",
       "      <td>700.085000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89.000000</td>\n",
       "      <td>38.578759</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>58.460000</td>\n",
       "      <td>25.050342</td>\n",
       "      <td>90.280000</td>\n",
       "      <td>38.040000</td>\n",
       "      <td>82.100000</td>\n",
       "      <td>1698.440000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age         BMI     Glucose     Insulin        HOMA      Leptin  \\\n",
       "count  116.000000  116.000000  116.000000  116.000000  116.000000  116.000000   \n",
       "mean    57.301724   27.582111   97.793103   10.012086    2.694988   26.615080   \n",
       "std     16.112766    5.020136   22.525162   10.067768    3.642043   19.183294   \n",
       "min     24.000000   18.370000   60.000000    2.432000    0.467409    4.311000   \n",
       "25%     45.000000   22.973205   85.750000    4.359250    0.917966   12.313675   \n",
       "50%     56.000000   27.662416   92.000000    5.924500    1.380939   20.271000   \n",
       "75%     71.000000   31.241442  102.000000   11.189250    2.857787   37.378300   \n",
       "max     89.000000   38.578759  201.000000   58.460000   25.050342   90.280000   \n",
       "\n",
       "       Adiponectin    Resistin        MCP.1  Classification  \n",
       "count   116.000000  116.000000   116.000000      116.000000  \n",
       "mean     10.180874   14.725966   534.647000        1.551724  \n",
       "std       6.843341   12.390646   345.912663        0.499475  \n",
       "min       1.656020    3.210000    45.843000        1.000000  \n",
       "25%       5.474283    6.881763   269.978250        1.000000  \n",
       "50%       8.352692   10.827740   471.322500        2.000000  \n",
       "75%      11.815970   17.755207   700.085000        2.000000  \n",
       "max      38.040000   82.100000  1698.440000        2.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing so, for the second dataset and its dataframe.\n",
    "\n",
    "# It is also optional to display it as a statistical distribution.\n",
    "# So that, before applying training and testing methods, \n",
    "# it would be essential to have a look at it much more detailed.\n",
    "\n",
    "df_d2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f021f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before the train and test set procedure takes place,\n",
    "# the 'Classification' column will be checked and distributed.\n",
    "\n",
    "# First, count how many values are there on the 'Classification' column.\n",
    "\n",
    "# can be performed, either, using the value_counts() method\n",
    "# under: class_distr = df_d1['Classification'].value_counts()\n",
    "\n",
    "# or declare a separate variable for the total amount of values.\n",
    "# (and perform the total() method for the final check)\n",
    "\n",
    "# total_classification = df_d2['Classification'].count()\n",
    "\n",
    "# In this case, a separate variable for the total values of the 'Classification' column will be declared.\n",
    "\n",
    "total_classification = df_d2['Classification'].count()\n",
    "\n",
    "# Then, count how many values are there for the classification value set to '1'\n",
    "# For this case, it will be detected on which rank the values may belong.\n",
    "# Hypotheticly, let's pretend the values set to '1', stand for \"breast cancer negative\" diagnosis,\n",
    "# which means, the patient does not have breast cancer.\n",
    "\n",
    "neg_count = df_d2[df_d2['Classification'] == 1].shape[0]\n",
    "\n",
    "# Next step, is to count how many values are there for the classification value set to '2'\n",
    "# For this case, it will be detected on which rank the values may belong.\n",
    "# Hypotheticly, let's pretend the values set to '2', stand for \"breast cancer positive\" diagnosis,\n",
    "# which means, the patient does have breast cancer.\n",
    "\n",
    "pos_count = df_d2[df_d2['Classification'] == 2].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c1eeabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total values are:  116\n",
      "Negative values are:  52\n",
      "Positive values are:  64\n"
     ]
    }
   ],
   "source": [
    "# Once all values have been counted, the current objective is to print them,\n",
    "# and see the results of each value registered.\n",
    "\n",
    "# Beginning with all values in total \n",
    "# (positive and negative, 1 and 2, altogether)\n",
    "\n",
    "print(\"Total values are: \", total_classification)\n",
    "\n",
    "# then carry on with each value individually\n",
    "\n",
    "# negative values (or values set to 1)\n",
    "\n",
    "print(\"Negative values are: \", neg_count)\n",
    "\n",
    "# positive values (or values set to 2)\n",
    "\n",
    "print(\"Positive values are: \", pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7a1ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second (and current) dataset has been read.\n",
    "\n",
    "# Information and contents of the second dataset have been fetched.\n",
    "# An attempt to pre-process it, will be made.\n",
    "\n",
    "# It will be split into a train and test set,\n",
    "# in order for the values to be trained (and tested, each)\n",
    "# for the train-test process to take place.\n",
    "\n",
    "# It will be split into labels and features.\n",
    "# Features, are represented under the x variable. (in this case, x2, as it stands for the second dataset variable)\n",
    "# Labels (aka the target variable) are represented under the y variable. (in this case, y2, as it stands for the second dataset y variable)\n",
    "\n",
    "# On the command below, the x2 and y2 variables, will determine each, features and labels.\n",
    "\n",
    "# solution without converting each value of the 'Classification' column into numerical values\n",
    "\n",
    "x2 = df_d2.drop('Classification', axis=1) # Features' variable\n",
    "y2 = df_d2['Classification'] # Target variable\n",
    "\n",
    "# (for the y1 variable, we can also declare y1 = df_d1.diagnosis without putting the column name in brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "958553b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step, is to split the data into training and testing set.\n",
    "# Features and labels have been represented and declared in two variables each;\n",
    "# x for the features, and y for the labels/target variable.\n",
    "\n",
    "# in the above case, x2 for the features\n",
    "# and y2 for the labels of the df_d1 dataframe.\n",
    "\n",
    "# Once split into labels and features, its logic will be implemented \n",
    "# on a scale of 90-10; meant by, 90% for training, and 10% for testing.\n",
    "# Its test size, will be set to 0.1.\n",
    "\n",
    "# Out of 116 rows and 10 columns, the random state is set to 42 by default.\n",
    "# There will not be any further change to its training value.\n",
    "\n",
    "# In this process, the data will have to be split into training and testing variables\n",
    "# under x2_train, x2_test, for the x axis\n",
    "# and y2_train, y2_test, for the y axis.\n",
    "\n",
    "# the train_test_split function, has been imported above,\n",
    "# and before reading the datasets in CSV format.\n",
    "\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.1, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce89f960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2 train shape is:  (104, 9)\n",
      "x2 test shape is:  (12, 9)\n",
      "y2 train shape is:  (104,)\n",
      "y2 test shape is:  (12,)\n"
     ]
    }
   ],
   "source": [
    "# Optionally, the train and test values could be printed.\n",
    "# Either as a set (each for the x1 and y1 variables),\n",
    "# or individually\n",
    "\n",
    "# In this case, set values will be examined.\n",
    "# Examining the first dataset, therefore the train and test variables, \n",
    "# have been declared as x2 and y2.\n",
    "\n",
    "print(\"x2 train shape is: \", x2_train.shape)\n",
    "print(\"x2 test shape is: \", x2_test.shape)\n",
    "\n",
    "print(\"y2 train shape is: \", y2_train.shape)\n",
    "print(\"y2 test shape is: \", y2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "042ab760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 9)\n",
      "(116,)\n"
     ]
    }
   ],
   "source": [
    "# After the decision tree has been made, an attempt on performing a stratified k-fold cross-validation process, will be made.\n",
    "# Cross validation, however, requires support vector machines module, which may be used due to probabilistic values\n",
    "# It doesn't only apply to the usual cross-validation process (without folding) \n",
    "# as well as the K-fold cross-validation (pure) and stratified K-fold cross-validation processes.\n",
    "\n",
    "# In this case, a stratified ten-fold cross-validation process is being examined.\n",
    "# Which means, the K variable will be set to 10 as its value.\n",
    "# (the K variable stands for folding)\n",
    "\n",
    "# Optionally, the shapes for the x2 and y2 variables, will be printed.\n",
    "\n",
    "# x2.shape\n",
    "# y2.shape\n",
    "\n",
    "print(x2.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99dfcc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stratified K-Fold process held for the second dataset.\n",
    "# Splits will be set to 10 initially.\n",
    "\n",
    "skf2 = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# Get the number of splits for the process.\n",
    "\n",
    "skf2.get_n_splits(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05ae9cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "# Print the status of the stratified K-Fold process \n",
    "# before the actual process takes place.\n",
    "\n",
    "print(skf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fc16010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize values used for true positives, true negatives\n",
    "# false positives and false negatives\n",
    "\n",
    "# each value will be set to 0\n",
    "\n",
    "# begin with true positives\n",
    "\n",
    "tp2_total = 0\n",
    "\n",
    "# false positives\n",
    "\n",
    "fp2_total = 0\n",
    "\n",
    "# true negatives\n",
    "\n",
    "tn2_total = 0\n",
    "\n",
    "# false negatives\n",
    "\n",
    "fn2_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19c10afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, testing and performing a k-fold cross validation process on the second dataframe's contents,\n",
    "# an attempt on creating decision trees for each fold generated, will be made.\n",
    "\n",
    "# Beginning with (and examining) Decision Trees.\n",
    "# As long as the necessary modules have been imported, the model will now be defined for the experiment to take place.\n",
    "\n",
    "# This case will be featured inside a loop, \n",
    "# so that the tree will be plotted right after the stratified K-fold cross-validation module\n",
    "# having already taken place.\n",
    "\n",
    "# Since none of the regression methods worked, the next attempt will be made on creating decision tree(s) for each output.\n",
    "# (catches an error without converting any value of the 'diagnosis' column into numerical values)\n",
    "# this case will examine the trained data\n",
    "\n",
    "# define the decision tree classifier\n",
    "# and initialize BEFORE the loop\n",
    "\n",
    "dtc2 = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2543d098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "  Train: index=[  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19\n",
      "  20  21  22  23  24  25  26  28  29  30  31  32  33  34  35  36  37  39\n",
      "  40  41  42  43  45  46  47  48  49  51  52  53  54  55  56  57  59  60\n",
      "  61  62  63  64  65  66  67  68  70  71  72  73  75  76  77  78  79  81\n",
      "  82  83  84  85  86  87  89  90  91  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "  Test:  index=[ 1 16 27 38 44 50 58 69 74 80 88 92]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.80      0.62         5\n",
      "           2       0.75      0.43      0.55         7\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.62      0.61      0.58        12\n",
      "weighted avg       0.65      0.58      0.57        12\n",
      " \n",
      "\n",
      "Confusion matrix for Fold 1: \n",
      " [[4 1]\n",
      " [4 3]]\n",
      "Accuracy for fold 1:  0.5833333333333334\n",
      "Precision for fold 1:  [0.5  0.75]\n",
      "Recall for fold 1:  [0.8        0.42857143]\n",
      "F1-score for fold 1:  [0.61538462 0.54545455]\n",
      "Fold 2:\n",
      "  Train: index=[  0   1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  18  19\n",
      "  21  22  23  24  25  26  27  28  29  30  32  33  34  35  36  38  39  40\n",
      "  41  42  43  44  45  46  48  49  50  51  52  53  54  55  56  57  58  59\n",
      "  60  61  62  63  64  65  66  67  69  70  72  73  74  75  76  77  79  80\n",
      "  81  82  83  84  85  86  87  88  89  90  91  92  94  95  96  97  98  99\n",
      " 100 101 103 104 105 107 108 109 110 111 112 113 114 115]\n",
      "  Test:  index=[ 11  17  20  31  37  47  68  71  78  93 102 106]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.80      0.67         5\n",
      "           2       0.80      0.57      0.67         7\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.69      0.69      0.67        12\n",
      "weighted avg       0.70      0.67      0.67        12\n",
      " \n",
      "\n",
      "Confusion matrix for Fold 2: \n",
      " [[4 1]\n",
      " [3 4]]\n",
      "Accuracy for fold 2:  0.6666666666666666\n",
      "Precision for fold 2:  [0.57142857 0.8       ]\n",
      "Recall for fold 2:  [0.8        0.57142857]\n",
      "F1-score for fold 2:  [0.66666667 0.66666667]\n",
      "Fold 3:\n",
      "  Train: index=[  0   1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  23  24  25  27  28  29  30  31  32  33  35  36  37  38  39\n",
      "  40  41  42  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  62  63  64  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  98\n",
      "  99 100 101 102 105 106 107 108 109 110 112 113 114 115]\n",
      "  Test:  index=[  3  22  26  34  46  61  65  90  97 103 104 111]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.60      0.50         5\n",
      "           2       0.60      0.43      0.50         7\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.51      0.51      0.50        12\n",
      "weighted avg       0.53      0.50      0.50        12\n",
      " \n",
      "\n",
      "Confusion matrix for Fold 3: \n",
      " [[3 2]\n",
      " [4 3]]\n",
      "Accuracy for fold 3:  0.5\n",
      "Precision for fold 3:  [0.42857143 0.6       ]\n",
      "Recall for fold 3:  [0.6        0.42857143]\n",
      "F1-score for fold 3:  [0.5 0.5]\n",
      "Fold 4:\n",
      "  Train: index=[  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  19\n",
      "  20  22  23  24  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  56  57  58  59\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  78  79  80\n",
      "  81  82  83  84  85  86  87  88  90  91  92  93  94  95  96  97  98  99\n",
      " 100 101 102 103 104 106 107 108 109 110 111 112 113 114]\n",
      "  Test:  index=[  9  18  21  25  40  55  60  73  77  89 105 115]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.60      0.55         5\n",
      "           2       0.67      0.57      0.62         7\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.58      0.59      0.58        12\n",
      "weighted avg       0.60      0.58      0.59        12\n",
      " \n",
      "\n",
      "Confusion matrix for Fold 4: \n",
      " [[3 2]\n",
      " [3 4]]\n",
      "Accuracy for fold 4:  0.5833333333333334\n",
      "Precision for fold 4:  [0.5        0.66666667]\n",
      "Recall for fold 4:  [0.6        0.57142857]\n",
      "F1-score for fold 4:  [0.54545455 0.61538462]\n",
      "Fold 5:\n",
      "  Train: index=[  0   1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  24  25  26  27  30  31  32  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58\n",
      "  59  60  61  62  64  65  66  67  68  69  71  72  73  74  76  77  78  79\n",
      "  80  81  82  83  84  85  86  88  89  90  92  93  94  95  97  98  99 100\n",
      " 101 102 103 104 105 106 108 109 110 111 112 113 114 115]\n",
      "  Test:  index=[  7  23  28  29  51  63  70  75  87  91  96 107]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.80      0.62         5\n",
      "           2       0.75      0.43      0.55         7\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.62      0.61      0.58        12\n",
      "weighted avg       0.65      0.58      0.57        12\n",
      " \n",
      "\n",
      "Confusion matrix for Fold 5: \n",
      " [[4 1]\n",
      " [4 3]]\n",
      "Accuracy for fold 5:  0.5833333333333334\n",
      "Precision for fold 5:  [0.5  0.75]\n",
      "Recall for fold 5:  [0.8        0.42857143]\n",
      "F1-score for fold 5:  [0.61538462 0.54545455]\n",
      "Fold 6:\n",
      "  Train: index=[  0   1   2   3   4   6   7   8   9  10  11  12  14  15  16  17  18  20\n",
      "  21  22  23  25  26  27  28  29  30  31  32  33  34  35  36  37  38  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58  59\n",
      "  60  61  62  63  64  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  83  86  87  88  89  90  91  92  93  94  95  96  97  98 100\n",
      " 101 102 103 104 105 106 107 108 110 111 112 113 114 115]\n",
      "  Test:  index=[  5  13  19  24  39  56  67  82  84  85  99 109]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.60      0.55         5\n",
      "           2       0.67      0.57      0.62         7\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.58      0.59      0.58        12\n",
      "weighted avg       0.60      0.58      0.59        12\n",
      " \n",
      "\n",
      "Confusion matrix for Fold 6: \n",
      " [[3 2]\n",
      " [3 4]]\n",
      "Accuracy for fold 6:  0.5833333333333334\n",
      "Precision for fold 6:  [0.5        0.66666667]\n",
      "Recall for fold 6:  [0.6        0.57142857]\n",
      "F1-score for fold 6:  [0.54545455 0.61538462]\n",
      "Fold 7:\n",
      "  Train: index=[  0   1   2   3   5   7   8   9  11  12  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  31  32  34  35  37  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  55  56  57  58  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  80  82  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99 100\n",
      " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "  Test:  index=[ 4  6 10 33 36 54 59 79 81 83 95]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.60      0.55         5\n",
      "           2       0.67      0.57      0.62         7\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.58      0.59      0.58        12\n",
      "weighted avg       0.60      0.58      0.59        12\n",
      " \n",
      "\n",
      "Confusion matrix for Fold 7: \n",
      " [[3 2]\n",
      " [3 4]]\n",
      "Accuracy for fold 7:  0.5833333333333334\n",
      "Precision for fold 7:  [0.5        0.66666667]\n",
      "Recall for fold 7:  [0.6        0.57142857]\n",
      "F1-score for fold 7:  [0.54545455 0.61538462]\n",
      "Fold 8:\n",
      "  Train: index=[  0   1   2   3   4   5   6   7   9  10  11  12  13  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  36  37  38  39\n",
      "  40  41  42  43  44  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  68  69  70  71  73  74  75  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  99 101 102 103 104 105 106 107 108 109 111 113 114 115]\n",
      "  Test:  index=[  8  14  15  35  45  72  76  98 100 110 112]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.80      0.62         5\n",
      "           2       0.75      0.43      0.55         7\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.62      0.61      0.58        12\n",
      "weighted avg       0.65      0.58      0.57        12\n",
      " \n",
      "\n",
      "Confusion matrix for Fold 8: \n",
      " [[4 1]\n",
      " [4 3]]\n",
      "Accuracy for fold 8:  0.5833333333333334\n",
      "Precision for fold 8:  [0.5  0.75]\n",
      "Recall for fold 8:  [0.8        0.42857143]\n",
      "F1-score for fold 8:  [0.61538462 0.54545455]\n",
      "Fold 9:\n",
      "  Train: index=[  1   3   4   5   6   7   8   9  10  11  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
      "  39  40  41  44  45  46  47  48  49  50  51  54  55  56  58  59  60  61\n",
      "  63  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81\n",
      "  82  83  84  85  87  88  89  90  91  92  93  94  95  96  97  98  99 100\n",
      " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "  Test:  index=[ 0  2 12 42 43 52 53 57 62 64 86]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.60      0.50         5\n",
      "           2       0.60      0.43      0.50         7\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.51      0.51      0.50        12\n",
      "weighted avg       0.53      0.50      0.50        12\n",
      " \n",
      "\n",
      "Confusion matrix for Fold 9: \n",
      " [[3 2]\n",
      " [4 3]]\n",
      "Accuracy for fold 9:  0.5\n",
      "Precision for fold 9:  [0.42857143 0.6       ]\n",
      "Recall for fold 9:  [0.6        0.42857143]\n",
      "F1-score for fold 9:  [0.5 0.5]\n",
      "Fold 10:\n",
      "  Train: index=[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  31  33  34  35  36  37\n",
      "  38  39  40  42  43  44  45  46  47  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77\n",
      "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  95  96\n",
      "  97  98  99 100 102 103 104 105 106 107 109 110 111 112 115]\n",
      "  Test:  index=[ 30  32  41  48  49  66  94 101 108 113 114]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.60      0.50         5\n",
      "           2       0.60      0.43      0.50         7\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.51      0.51      0.50        12\n",
      "weighted avg       0.53      0.50      0.50        12\n",
      " \n",
      "\n",
      "Confusion matrix for Fold 10: \n",
      " [[3 2]\n",
      " [4 3]]\n",
      "Accuracy for fold 10:  0.5\n",
      "Precision for fold 10:  [0.42857143 0.6       ]\n",
      "Recall for fold 10:  [0.6        0.42857143]\n",
      "F1-score for fold 10:  [0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "# The actual process held for the second dataset, after printing all necessary stats.\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf2.split(x2, y2)):\n",
    "\n",
    "    # first, print the amount of folds\n",
    "    print(f\"Fold {i+1}:\")\n",
    "\n",
    "    # print the train indices\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "\n",
    "    # print the test indices\n",
    "    print(f\"  Test:  index={test_index}\")\n",
    "\n",
    "    # fit the classifier (after initialization)\n",
    "    # assign a new variable named dtc2_train as it appeals to the trained data\n",
    "    # on both x2 and y2 train sets\n",
    "\n",
    "    dtc2_train = dtc2.fit(x2_train, y2_train)\n",
    "\n",
    "    # next step is to make predictions on the test data\n",
    "\n",
    "    y2_pred = dtc2.predict(x2_test)\n",
    "\n",
    "    # next up, create a confusion matrix\n",
    "\n",
    "    cm2 = confusion_matrix(y2_test, y2_pred)\n",
    "\n",
    "    # optionally use classification report (much more detailed)\n",
    "\n",
    "    clr2 = classification_report(y2_test, y2_pred)\n",
    "\n",
    "    print (clr2, \"\\n\")\n",
    "    \n",
    "    # extract all variables set for true positives, true negatives, false positives and false negatives\n",
    "    # (generic use)\n",
    "\n",
    "    tp2, fp2, tn2, fn2 = cm2.ravel()\n",
    "\n",
    "    #print for each fold\n",
    "\n",
    "    print(f\"Confusion matrix for Fold {i+1}: \\n\", cm2)\n",
    "\n",
    "    # calculate metrics for each fold\n",
    "    # use a new variable\n",
    "\n",
    "    tp2_total += tp2\n",
    "    fp2_total += fp2\n",
    "    tn2_total += tn2\n",
    "    fn2_total += fn2\n",
    "\n",
    "    # next up, calculate metrics for accuracy, precision, recall and F-score\n",
    "    # on what was calculated during the first set of the ten-fold cross-validation process.\n",
    "\n",
    "    #accuracy\n",
    "    acc2 = accuracy_score(y2_test, y2_pred)\n",
    "    print(f\"Accuracy for fold {i+1}: \", acc2)\n",
    "\n",
    "    # precision\n",
    "\n",
    "    pr2 = precision_score(y2_test, y2_pred, average=None)\n",
    "    print(f\"Precision for fold {i+1}: \", pr2)\n",
    "\n",
    "    # recall\n",
    "\n",
    "    rec2 = recall_score(y2_test, y2_pred, average=None)\n",
    "    print(f\"Recall for fold {i+1}: \", rec2)\n",
    "\n",
    "    # F1-score\n",
    "\n",
    "    fsc2 = f1_score(y2_test, y2_pred, average=None)\n",
    "    print(f\"F1-score for fold {i+1}: \", fsc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd4d63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
